Oct 16, 2010
 
Exploring Tesseract-OCR code to try to identify major functionality. Also spending more time with pyTesser, seeing how to hook python up more directly with tesseract, and still looking into ocropus. I may just have to sit down and write a few programs to see what the API provides.
 
Also, finding appropriate STEP viewer for Linux, and looking into converting output to AP201 format. I’ll need a large improvement in recognition before I can start parsing it efficiently. Right now, I may be able to pull 2% - 5% of total information from scanned images into AP201.
 
I’ve also discovered that it is more effective to leave the characters in the drawings and use tessboxes to edit the box file with the additional context. 
 
Using a list of all words contained in image 6201018-02, ocropus managed to produce only two matches in scanning image 6201019-01. After training on the 3000+ char training set, 12 matches were produced. The default english language in tesseract managed 19 matches, but simply training on the original 6201018-02 for a couple hours yielded 27 matches. (From a wordlist of 46 words, and probably 350+ desirable words in the image)
 
Using this list of matched words, we can highlight and position these words to provide some context for the rest of the document. 
 
Likely the best course of action: combine several images, run tesseract on them as they are, produce a box file with  >10,000 legitimate matches, edit and correct this box file!
 
Legitimate match - Only character and select punctuation matches. 
 
Need to do: Research the best way to combine boxes, or separate boxes for accuracy. 
 
 
 
Emailed Shane: 
 
Hey Shane,
How's work coming? I've made a few discoveries after training with tesseract for a while on the larger training sets I built. Mainly, overall accuracy seemed to drop after the training. This is based off the number of 'terms' I could recognize after parsing the output where terms are just words from the original images. (just 6201018-02 right now)
 
I did some more digging into the tools, and found tessboxes is actually pretty sweet for editing box files, and makes it pretty trivial to train directly from the original images. I tried this on 6201018-02, and it took me maybe a 1.5hrs to get through it, and I could've done a better job combining/splitting boxes if I'd spent longer on it. Anyway, I took the data from that training session, and saw the number of recognizable terms jump up significantly.
 
Results: (From a wordlist of 46 words from 6201018-02, and probably 350+ desirable words in the image 6201019-01)
 
Ocropus managed to produce only two term matches in scanning image 6201019-01. 
After training on the 3000+ char training set, 12 matches were produced. 
The default english language in tesseract managed 19 matches, 
but simply training on the original 6201018-02 for a couple hours yielded 27 term matches.
 
I plan on doing some more testing, but it looks to me like the more context around the letters, the better. I guess at some point I'll do a full box edit for 6201019-01 using tessboxes, and see what kind of results I get.
 
I'm also starting work on dictionary matching. I figured a spell-checker might be a good place to start. Also, I was thinking about ways to use multiple tesseract languages, and scan through multiple outputs to increase accuracy in one output.
 
Apparently we have to do the dictionary work, and the parsing into step files. (iso 10303 ap201) I talked to McGraw about Mooney's work, and it sounds like he may only be providing a list of terms for the dictionary?
 
Anyway, just letting you know where I'm at right now.
 
--
-Forrest
